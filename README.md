# Research-Contributions
# Examining Speaker Bias in LLM Based on Prompts in African American Vernacular English vs. Standard American English 
[Examining Speaker Bias in LLM Based on Prompts in African American.pdf](https://github.com/user-attachments/files/18235253/Examining.Speaker.Bias.in.LLM.Based.on.Prompts.in.African.American.pdf)

  -Explored covert bias in LLMs using ChatGPT-4o Mini, Gemini 1.5, and Llama 3.2 as case studies
  
  -Investigated covert bias in these models by analyzing numerical values assigned to speaker characteristics (Intelligence, kindness, sophistication, aggression, emotional,      laziness, factual) through direct and indirect comparison of AAVE and SAE prompts
  
  -Conducted counterfactual fine-tuning on Llama 3.2 to evaluate the persistence of biases in fine-tuned language model. 
# Fine-Tuning Multimodal Foundation Models for Dementia [Fine-Tuning Multimodal Foundation Models for Dementia Diagnosis.pdf](https://github.com/user-attachments/files/18235249/Fine-Tuning.Multimodal.Foundation.Models.for.Dementia.Diagnosis.pdf)
 Diagnosis
  -Evaluated foundation models for dementia classification, focusing on performance and resource efficiency compared to specialized models.
  
  -Fine-tuned the LLaVA model with MRI data and benchmarked its performance against the baseline model - EfficientNet 
  
  -Determined the minimum training data required for the LLaVA model to achieve results comparable to the full dataset and baseline model.
  
  -Validated model robustness and generalizability using unseen datasets (ADNI) and cross-evaluations between OASIS and ADNI datasets.

